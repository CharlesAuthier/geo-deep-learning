# @package _global_
inference:
  input_stac_item: data/STAC_SpaceNet1_AOI_2/SpaceNet_AOI_2_Las_Vegas-056155973080_01_P001-WV03.json
  root_dir: ${general.raw_data_dir}
  output_name: SpaceNet_AOI_2_Las_Vegas-056155973080_01_P001-WV03_pred  # name of output file to write in root
  state_dict_path: saved_model/manet_pretrained_bds3_cls1.pth.tar
  download_data: False  # if True, will download local copy of imagery and infer from this copy rather than from web
                        # if network bandwidth is good, no need to download as only windows are downloaded to infer
  save_heatmap: True
  batch_size: # if empty, will be calculated automatically to fill "auto_batch_size_threshold" % of GPU Ram
  chunk_size: # Defaults to 512
  auto_batch_size_threshold: 90

  tta_merge_mode: max  # Test time augmentation merge mode. See: https://github.com/qubvel/ttach#merge-modes
  tta_transforms: horizontal_flip  # Test-time augmentation tranforms. See: https://github.com/qubvel/ttach#transforms
  postprocess: True  # if True, will proceed to postprocessing directly after inference  # TODO is this flag necessary?

  # GPU parameters
  gpu: 1  # Number of gpus to use. TODO implement >1
  max_used_perc: ${training.max_used_perc}  # If RAM usage of detected GPU exceeds this percentage, it will be ignored
  max_used_ram: ${training.max_used_ram}  # If GPU's usage exceeds this percentage, it will be ignored
